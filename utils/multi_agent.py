import asyncio
import os
import json
import logging
from typing import Dict, Any, Optional, List
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import HumanMessage, AIMessage
from langchain_mcp_adapters.client import MultiServerMCPClient

from adapter.openai_api import model
from database.memory_session import get_session_history
from database.models.mcp import McpTool
from utils.tools.interpreter import Interpreter, getTime
from utils.tools.retriever import get_retriever_tool
from utils.tools.code_assistant import (
    code_analyzer,
    code_generator,
    code_fixer,
    code_documentation,
    dependency_analyzer
)
from utils.tools.code_reviewer import (
    code_quality_check,
    security_review,
    best_practices_advisor
)

def convert_mcp_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    è½¬æ¢MCPé…ç½®æ ¼å¼ä»¥é€‚é…langchain-mcp-adaptersè¦æ±‚
    å¤„ç†ä¸¤ç§æ ¼å¼ï¼š
    1. ç›´æ¥çš„æœåŠ¡å™¨é…ç½®æ ¼å¼ï¼ˆæ­£ç¡®æ ¼å¼ï¼‰
    2. åŒ…å«mcpServersåŒ…è£…çš„æ ¼å¼ï¼ˆéœ€è¦è½¬æ¢ï¼‰
    """
    # å¦‚æœé…ç½®å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼ï¼ˆç›´æ¥åŒ…å«æœåŠ¡å™¨é…ç½®ï¼‰ï¼Œç›´æ¥è¿”å›
    if not config:
        return {}
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«mcpServersåŒ…è£…çš„æ ¼å¼
    if 'mcpServers' in config and isinstance(config['mcpServers'], dict):
        # è½¬æ¢æ ¼å¼ï¼šç§»é™¤mcpServersåŒ…è£…å¹¶è½¬æ¢typeä¸ºtransport
        converted = {}
        for server_name, server_config in config['mcpServers'].items():
            # åˆ›å»ºè½¬æ¢åçš„é…ç½®
            converted_config = {}
            for key, value in server_config.items():
                # å°†typeè½¬æ¢ä¸ºtransport
                if key == 'type':
                    converted_config['transport'] = value
                else:
                    converted_config[key] = value
            converted[server_name] = converted_config
        return converted
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„æœåŠ¡å™¨é…ç½®ä½†ä½¿ç”¨äº†typeè€Œä¸æ˜¯transport
    converted = {}
    for key, value in config.items():
        if isinstance(value, dict):
            # è½¬æ¢typeä¸ºtransport
            converted_config = {}
            for sub_key, sub_value in value.items():
                if sub_key == 'type':
                    converted_config['transport'] = sub_value
                else:
                    converted_config[sub_key] = sub_value
            converted[key] = converted_config
        else:
            converted[key] = value
    
    return converted

# é»˜è®¤çŸ¥è¯†åº“å·¥å…· - åªä½œä¸ºå¤‡ç”¨
default_retriever_tool = get_retriever_tool(path="./vector_db/know_db")

# åŸºç¡€å·¥å…·é›†ï¼Œä¸åŒ…å«çŸ¥è¯†åº“æ£€ç´¢å·¥å…·
base_tools = [
    # åŸºæœ¬å·¥å…·
    getTime,
    Interpreter,

    # ä»£ç åŠ©æ‰‹å·¥å…·
    code_analyzer,
    code_generator,
    code_fixer,
    code_documentation,
    dependency_analyzer,

    # ä»£ç å®¡æŸ¥å·¥å…·
    code_quality_check,
    security_review,
    best_practices_advisor
]

# ç³»ç»Ÿæç¤º
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç åŠ©æ‰‹ï¼Œæ“…é•¿ç¼–ç¨‹å’Œè§£å†³æŠ€æœ¯é—®é¢˜ã€‚ä½ å¯ä»¥ï¼š
1. åˆ†æä»£ç ç»“æ„å’Œè¯­æ³•
2. æ ¹æ®æè¿°ç”Ÿæˆä»£ç 
3. ä¿®å¤ä»£ç ä¸­çš„é”™è¯¯
4. æ£€æŸ¥ä»£ç è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®
5. å®¡æŸ¥ä»£ç å®‰å…¨æ€§
6. æä¾›ç¼–ç¨‹è¯­è¨€æœ€ä½³å®è·µ
7. æ‰§è¡Œç®€å•çš„ä»£ç ç¤ºä¾‹

è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ï¼Œå¹¶å……åˆ†åˆ©ç”¨æä¾›çš„å·¥å…·æ¥è§£å†³ç”¨æˆ·çš„é—®é¢˜ã€‚

å½“éœ€è¦ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ—¶ï¼Œä½ ä¼šè°ƒç”¨ç›¸åº”çš„ä¸“å®¶ä»£ç†æ¥ååŠ©å¤„ç†ã€‚
"""

# ä¸“å®¶ä»£ç†ç³»ç»Ÿæç¤º
EXPERT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨çš„{expertise}ä¸“å®¶ä»£ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯ååŠ©ä¸»ä»£ç†è§£å†³{expertise}ç›¸å…³çš„é—®é¢˜ã€‚
è¯·ä¸“æ³¨äºä½ çš„ä¸“ä¸šé¢†åŸŸï¼Œå¹¶æä¾›å‡†ç¡®ã€è¯¦ç»†çš„è§£ç­”ã€‚
"""

def get_expert_agent(expertise: str):
    """åˆ›å»ºä¸“å®¶ä»£ç†"""
    # ä¸“å®¶ç‰¹å®šå·¥å…·ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä¸åŒä¸“å®¶æœ‰ä¸åŒçš„å·¥å…·ï¼‰
    expert_tools = base_tools.copy()

    # åˆ›å»ºä¸“å®¶æç¤ºæ¨¡æ¿
    expert_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", EXPERT_SYSTEM_PROMPT.format(expertise=expertise)),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
            MessagesPlaceholder(variable_name='agent_scratchpad'),
        ]
    )

    # åˆ›å»ºä¸“å®¶ä»£ç†å’Œæ‰§è¡Œå™¨
    expert_agent = create_tool_calling_agent(model, expert_tools, expert_prompt)
    expert_executor = AgentExecutor(agent=expert_agent, tools=expert_tools, verbose=True)

    return expert_executor

async def load_mcp_tools(mcp_config_path: str = None, user_id: int = None) -> List[Any]:
    """ä»JSONé…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½MCPå·¥å…·"""
    try:
        mcp_configs = {}

        # å¦‚æœæä¾›äº†ç”¨æˆ·IDï¼Œä¼˜å…ˆä»æ•°æ®åº“åŠ è½½
        if user_id is not None:
            # æŸ¥è¯¢ç”¨æˆ·æ¿€æ´»çš„MCPå·¥å…·
            tools = McpTool.select().where(
                (McpTool.user_id == user_id) &
                (McpTool.is_active == 1)
            )

            # åˆå¹¶æ‰€æœ‰å·¥å…·é…ç½®ï¼Œå¹¶è½¬æ¢æ ¼å¼
            for tool in tools:
                # è½¬æ¢é…ç½®æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                converted_config = convert_mcp_config(tool.config)
                mcp_configs.update(converted_config)

        # å¦‚æœæ²¡æœ‰ä»æ•°æ®åº“åŠ è½½åˆ°é…ç½®ä¸”æä¾›äº†æ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä»æ–‡ä»¶åŠ è½½
        if not mcp_configs and mcp_config_path and os.path.exists(mcp_config_path):
            with open(mcp_config_path, 'r', encoding='utf-8') as f:
                file_configs = json.load(f)
                # è½¬æ¢é…ç½®æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                converted_file_configs = convert_mcp_config(file_configs)
                mcp_configs.update(converted_file_configs)

        # å¦‚æœæœ‰é…ç½®åˆ™åˆ›å»ºMCPå®¢æˆ·ç«¯
        if mcp_configs:
            # åˆ›å»ºMCPå®¢æˆ·ç«¯
            client = MultiServerMCPClient(mcp_configs)
            # è·å–å·¥å…·
            tools = await client.get_tools()
            return tools
        else:
            # æ²¡æœ‰MCPé…ç½®
            return []
    except Exception as e:
        logging.error(f"åŠ è½½MCPå·¥å…·æ—¶å‡ºé”™: {e}")
        return []

async def get_multi_agent_executor(user_id=None, collection_name=None, mcp_config_path=None):
    """
    è·å–å¤šä»£ç†æ‰§è¡Œå™¨
    :param user_id: ç”¨æˆ·ID
    :param collection_name: çŸ¥è¯†åº“é›†åˆåç§°
    :param mcp_config_path: MCPé…ç½®æ–‡ä»¶è·¯å¾„
    :return: é…ç½®å¥½çš„ä»£ç†æ‰§è¡Œå™¨
    """
    # ç¡®å®šä½¿ç”¨çš„æ£€ç´¢å·¥å…·
    if user_id and collection_name:
        # ç”¨æˆ·ç‰¹å®šçŸ¥è¯†åº“è·¯å¾„
        user_kb_path = os.path.join("./vector_db", f"user_{user_id}", collection_name)
        if os.path.exists(user_kb_path):
            # ç”¨æˆ·ç‰¹å®šçŸ¥è¯†åº“å­˜åœ¨ï¼Œä½¿ç”¨ç”¨æˆ·çš„çŸ¥è¯†åº“
            retriever_tool = get_retriever_tool(path=user_kb_path, collection_name=collection_name)
        else:
            # ç”¨æˆ·ç‰¹å®šçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“
            retriever_tool = default_retriever_tool
    else:
        # æ²¡æœ‰æŒ‡å®šç”¨æˆ·IDæˆ–é›†åˆåç§°ï¼Œä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“
        retriever_tool = default_retriever_tool

    # åˆå¹¶åŸºç¡€å·¥å…·
    tools = base_tools + [retriever_tool]

    # åŠ è½½MCPå·¥å…·ï¼ˆä¼˜å…ˆä»æ•°æ®åº“åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»æ–‡ä»¶åŠ è½½ï¼‰
    mcp_tools = []
    if user_id:
        # å¦‚æœæœ‰ç”¨æˆ·IDï¼Œå°è¯•ä»æ•°æ®åº“åŠ è½½MCPå·¥å…·
        mcp_tools = await load_mcp_tools(mcp_config_path, int(user_id))
    elif mcp_config_path:
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·IDä½†æœ‰é…ç½®è·¯å¾„ï¼Œä»æ–‡ä»¶åŠ è½½
        mcp_tools = await load_mcp_tools(mcp_config_path)

    tools.extend(mcp_tools)

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
            MessagesPlaceholder(variable_name='agent_scratchpad'),
        ]
    )

    # åˆ›å»ºä»£ç†å’Œæ‰§è¡Œå™¨ï¼Œä¼˜åŒ–æµå¼é…ç½®
    multi_agent = create_tool_calling_agent(model, tools, prompt)
    agent_executor = AgentExecutor(
        agent=multi_agent,
        tools=tools,
        verbose=True,
        streaming=True,  # ç¡®ä¿å¼€å¯æµå¼è¾“å‡º
        handle_parsing_errors=True,
        stream_runnable=True,  # ç¡®ä¿Runnableä¹Ÿæ”¯æŒæµå¼
        return_intermediate_steps=True,  # è¿”å›ä¸­é—´æ­¥éª¤ä»¥æ”¯æŒå·¥å…·è°ƒç”¨è¿½è¸ª
        max_iterations=10,  # é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°
        early_stopping_method="generate"  # ä¼˜åŒ–åœæ­¢ç­–ç•¥
    )

    return agent_executor

async def chat_with_multi_agent_original(msg, session_id, user_id=None, collection_name=None, mcp_config_path=None):
    """
    ä½¿ç”¨å¤šä»£ç†ä¸ç”¨æˆ·èŠå¤©
    :param msg: ç”¨æˆ·æ¶ˆæ¯
    :param session_id: ä¼šè¯ID
    :param user_id: ç”¨æˆ·ID
    :param collection_name: çŸ¥è¯†åº“é›†åˆåç§°
    :param mcp_config_path: MCPé…ç½®æ–‡ä»¶è·¯å¾„
    :return: ä»£ç†å›å¤
    """
    # è·å–å¤šä»£ç†æ‰§è¡Œå™¨
    agent_executor = await get_multi_agent_executor(user_id, collection_name, mcp_config_path)

    # æ·»åŠ èŠå¤©å†å²
    agent_with_chat_history = RunnableWithMessageHistory(
        agent_executor,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )

    # è°ƒç”¨ä»£ç†
    res = agent_with_chat_history.invoke(
        {"question": msg},
        config={"configurable": {"session_id": session_id}},
    )

    return res['output']

# æ”¹è¿›çš„æµå¼ç‰ˆæœ¬çš„èŠå¤©å‡½æ•°ï¼Œåˆ©ç”¨LangChainçš„äº‹ä»¶ç³»ç»Ÿå®ç°çœŸæ­£çš„æµå¼è¾“å‡º
async def stream_chat_with_multi_agent(msg, session_id, user_id=None, collection_name=None, mcp_config_path=None):
    """
    ä½¿ç”¨å¤šä»£ç†ä¸ç”¨æˆ·èŠå¤©ï¼ˆä¼˜åŒ–çš„æµå¼ç‰ˆæœ¬ï¼‰
    ä½¿ç”¨LangChainçš„äº‹ä»¶ç³»ç»Ÿå®ç°çœŸæ­£çš„å­—ç¬¦çº§æµå¼è¾“å‡º
    :param msg: ç”¨æˆ·æ¶ˆæ¯
    :param session_id: ä¼šè¯ID
    :param user_id: ç”¨æˆ·ID
    :param collection_name: çŸ¥è¯†åº“é›†åˆåç§°
    :param mcp_config_path: MCPé…ç½®æ–‡ä»¶è·¯å¾„
    :yield: ä»£ç†å›å¤çš„æµå¼æ•°æ®ï¼ŒåŒºåˆ†å·¥å…·è°ƒç”¨å’Œæ¨¡å‹å“åº”
    """
    # è·å–å¤šä»£ç†æ‰§è¡Œå™¨
    agent_executor = await get_multi_agent_executor(user_id, collection_name, mcp_config_path)

    # æ·»åŠ èŠå¤©å†å²
    agent_with_chat_history = RunnableWithMessageHistory(
        agent_executor,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )

    # å­˜å‚¨å·¥å…·è°ƒç”¨ä¿¡æ¯å’Œæµå¼æ–‡æœ¬
    tool_calls = []
    streaming_text = ""
    is_streaming_response = False
    
    try:
        # ä½¿ç”¨LangChainçš„é«˜çº§äº‹ä»¶APIè¿›è¡Œæ›´ç²¾ç»†çš„æµå¼æ§åˆ¶
        async for event in agent_with_chat_history.astream_events(
            {"question": msg},
            config={
                "configurable": {"session_id": session_id},
                "callbacks": []  # ä½¿ç”¨é»˜è®¤å›è°ƒ
            },
            version="v2"  # ä½¿ç”¨æ›´æ–°çš„äº‹ä»¶APIç‰ˆæœ¬
        ):
            event_type = event.get("event", "")
            event_name = event.get("name", "")
            event_data = event.get("data", {})
            
            # è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰äº‹ä»¶ç±»å‹
            logging.debug(f"Event: {event_type}, Name: {event_name}, Data keys: {list(event_data.keys()) if isinstance(event_data, dict) else 'not dict'}")
            
            # å¤„ç†ä¸åŒç±»å‹çš„äº‹ä»¶
            if event_type == "on_tool_start":
                # å·¥å…·å¼€å§‹è°ƒç”¨
                tool_name = event_name
                tool_input = event_data.get("input", {})
                
                logging.info(f"ğŸ”§ å·¥å…·å¼€å§‹è°ƒç”¨: {tool_name}, è¾“å…¥: {tool_input}")
                
                tool_call_info = {
                    "type": "tool_call",
                    "name": tool_name,
                    "input": tool_input,
                    "status": "started"
                }
                tool_calls.append(tool_call_info)
                yield f"[TOOL_CALL_START]{json.dumps(tool_call_info, ensure_ascii=False)}[TOOL_CALL_END]"
            
            elif event_type == "on_tool_end":
                # å·¥å…·è°ƒç”¨ç»“æŸ
                tool_name = event_name
                tool_output = event_data.get("output", "")
                
                logging.info(f"ğŸ“‹ å·¥å…·è°ƒç”¨ç»“æŸ: {tool_name}, è¾“å‡º: {tool_output}")
                
                tool_result_info = {
                    "type": "tool_result",
                    "name": tool_name,
                    "output": str(tool_output),
                    "status": "completed"
                }
                yield f"[TOOL_RESULT_START]{json.dumps(tool_result_info, ensure_ascii=False)}[TOOL_RESULT_END]"
                
                # æ›´æ–°tool_callsä¸­å¯¹åº”çš„å·¥å…·çŠ¶æ€
                for tool_call in tool_calls:
                    if tool_call.get("name") == tool_name:
                        tool_call["status"] = "completed"
                        tool_call["output"] = str(tool_output)
                        break
            
            elif event_type == "on_llm_stream":
                # LLMæµå¼è¾“å‡º - è¿™æ˜¯çœŸæ­£çš„å­—ç¬¦çº§æµå¼è¾“å‡º
                is_streaming_response = True
                chunk_content = event_data.get("chunk", {})
                
                # å¤„ç†OpenAIæ ¼å¼çš„æµå¼chunk
                if hasattr(chunk_content, 'content') and chunk_content.content:
                    content = chunk_content.content
                    streaming_text += content
                    yield f"[MODEL_RESPONSE]{content}"
                
                # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„chunk
                elif isinstance(chunk_content, str) and chunk_content:
                    streaming_text += chunk_content
                    yield f"[MODEL_RESPONSE]{chunk_content}"
                
                # å¤„ç†å­—å…¸æ ¼å¼çš„chunk
                elif isinstance(chunk_content, dict):
                    if "content" in chunk_content and chunk_content["content"]:
                        content = chunk_content["content"]
                        streaming_text += content
                        yield f"[MODEL_RESPONSE]{content}"
                    elif "text" in chunk_content and chunk_content["text"]:
                        text = chunk_content["text"]
                        streaming_text += text
                        yield f"[MODEL_RESPONSE]{text}"
            
            elif event_type == "on_llm_end":
                # LLMè¾“å‡ºç»“æŸ
                # å¦‚æœstreaming_textä¸ºç©ºï¼Œå°è¯•ä»event_dataä¸­è·å–å®Œæ•´è¾“å‡º
                if not streaming_text:
                    output = event_data.get("output", {})
                    if hasattr(output, 'content'):
                        streaming_text = output.content
                        # å®ç°å­—ç¬¦çº§æµå¼è¾“å‡º
                        if streaming_text and not is_streaming_response:
                            # å¦‚æœæ²¡æœ‰æ”¶åˆ°æµå¼chunkï¼Œæ¨¡æ‹Ÿå­—ç¬¦çº§è¾“å‡º
                            for i, char in enumerate(streaming_text):
                                yield f"[MODEL_RESPONSE]{char}"
                                # å¯é€‰ï¼šæ·»åŠ å°å»¶è¿Ÿæ¥æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
                                if i % 10 == 9:  # æ¯10ä¸ªå­—ç¬¦æ£€æŸ¥ä¸€ä¸‹
                                    await asyncio.sleep(0.01)  # éå¸¸å°çš„å»¶è¿Ÿ
                    elif isinstance(output, dict) and "content" in output:
                        streaming_text = output["content"]
                        if streaming_text and not is_streaming_response:
                            # æ¨¡æ‹Ÿå­—ç¬¦çº§è¾“å‡º
                            for i, char in enumerate(streaming_text):
                                yield f"[MODEL_RESPONSE]{char}"
                                if i % 10 == 9:
                                    await asyncio.sleep(0.01)
            
            elif event_type == "on_agent_action":
                # ä»£ç†åŠ¨ä½œï¼ˆä¸­é—´æ­¥éª¤ï¼‰
                action = event_data.get("action", {})
                if hasattr(action, 'tool') and hasattr(action, 'tool_input'):
                    intermediate_info = {
                        "type": "intermediate_step",
                        "name": action.tool,
                        "input": action.tool_input,
                        "log": getattr(action, 'log', '')
                    }
                    yield f"[INTERMEDIATE_START]{json.dumps(intermediate_info)}[INTERMEDIATE_END]"
            
            elif event_type == "on_chain_end":
                # é“¾ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯agent_executorçš„æœ€ç»ˆè¾“å‡º
                if event_name == "AgentExecutor":
                    output = event_data.get("output", {})
                    if isinstance(output, dict) and "output" in output:
                        final_output = output["output"]
                        # å¦‚æœæ²¡æœ‰é€šè¿‡æµå¼è·å–åˆ°å†…å®¹ï¼Œä½¿ç”¨æœ€ç»ˆè¾“å‡º
                        if not streaming_text and final_output:
                            # å®ç°å­—ç¬¦çº§æµå¼è¾“å‡º
                            for i, char in enumerate(final_output):
                                yield f"[MODEL_RESPONSE]{char}"
                                if i % 10 == 9:
                                    await asyncio.sleep(0.01)
        
        # å‘é€å·¥å…·è°ƒç”¨æ€»ç»“ä¿¡æ¯ - ä½†ä¸ä½œä¸ºMODEL_RESPONSEå‘é€
        if tool_calls:
            summary_info = {
                "type": "tool_summary",
                "tool_calls": tool_calls
            }
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†yieldï¼Œé¿å…å‡ºç°åœ¨ç”¨æˆ·æ¶ˆæ¯ä¸­
            logging.info(f"å·¥å…·è°ƒç”¨æ€»ç»“: {len(tool_calls)} ä¸ªå·¥å…·è¢«è°ƒç”¨")
    
    except Exception as e:
        logging.error(f"æµå¼èŠå¤©è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # å¦‚æœæµå¼å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹å®ç°
        logging.info("å›é€€åˆ°åŸå§‹æµå¼å®ç°")
        yield f"[MODEL_RESPONSE]æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚è®©æˆ‘é‡æ–°å°è¯•...\n\n"
        
        # å›é€€é€»è¾‘ï¼šä½¿ç”¨åŸå§‹çš„astreamæ–¹æ³•
        try:
            async for chunk in agent_with_chat_history.astream(
                {"question": msg},
                config={"configurable": {"session_id": session_id}},
            ):
                if isinstance(chunk, dict) and "output" in chunk and chunk["output"]:
                    # å¯¹å›é€€çš„è¾“å‡ºä¹Ÿå®ç°å­—ç¬¦çº§æµå¼
                    output_text = chunk["output"]
                    for i, char in enumerate(output_text):
                        yield f"[MODEL_RESPONSE]{char}"
                        if i % 5 == 4:  # å›é€€æ—¶ç¨å¾®å¿«ä¸€äº›
                            await asyncio.sleep(0.005)
                    break
        except Exception as fallback_error:
            logging.error(f"å›é€€å®ç°ä¹Ÿå¤±è´¥: {fallback_error}")
            yield f"[MODEL_RESPONSE]æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"